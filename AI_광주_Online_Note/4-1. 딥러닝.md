> [광주 인공지능 사관학교 | 딥러닝]([http://precourse.gj-aischool.com/lectures/10](http://precourse.gj-aischool.com/lectures/10))

### Deep Learning

* 딥러닝이란:

	인간의 신경망 이론을 이용한 인공신경망의 일종으로, 계층 구조로 구성되며 입력층과 출력층 사이에 하나 이상의 은닉층을 가지고 있는 **심층 신경망**을 이용하는 인공지능 분야이다.

* 인공신경망이란 :

	인간의 뇌 구조를 모방하여 모델링한 수학적 모델
![딥러닝](https://github.com/Deplim/DevNote/blob/master/Image/%EB%94%A5%EB%9F%AC%EB%8B%9D.png?raw=true)

* 뇌 구조

	* 뇌 구조를 아주 크게 나누면, 신경세포인 뉴런과 시냅스 혹은 가중치인 연결로 구분할 수 있다.

	* 뉴런의 입력은 다수이고, 출력은 하나이며, 여러 신경세포로부터 전달되어 온 신호들이 합산되어 출력된다.

	* 조건출력 : 합산된 값이 설정값 이상이면 출력 신호가 생기고, 이하이면 아예 생기지 않는다.

	* 수상돌기 : 입력

	* 세포체 : 입력합산 지점

	* 축삭 : 출력

	* 시냅스 : 다수의 뉴런 연결

	* 인공신경망 역시 노드들을 연결시키고 층을 만들며 이러한 연결 강도는 가중치로 처리됨.

* 출현 : 인공신경망은 1957년에서 프랭크 로젠블랫에 의해 고안된 퍼셉트론에서 시작하여

	1. Perceptron

	2. Multilayer perceptron

	3. Backpropagation (역전파)

	4. Recurrent Neural Network (RNN)

	5. 손글씨 문자인식 연구

	6. 딥러닝 용어 등장

	7. Drop Out 알고리즘

	의 과정을 거치며 지금의 딥러닝에 다다를 수 있었다.

* 딥러닝은 20세기 들어 충분한 하드웨어 성능과 그래픽 처리장치, 드롭아웃과도 같은 알고리즘의 발견으로 폭발적으로 향상되고있는 중임.

* CNN = 합성곱 신경망 (Convolutional Neural Network)

	* 이미지 인식에 주로 사용

	* 그림의 화풍을 인식해 사용

	* SegNet

* RSS = 순환 신경망
	(Recurrent Neural Network)

	* 음성 혹은 글자와 관련딘 부분에서 사용

	* 음성과 텍스트의 공통점은 길이가 가변적이라는 것

	* LSTM. GRU 등으로 발전

----------------------------------

### 이론

* 초창기 머신러닝

* 맥컬론-피츠(MCP) : 1943 년 간소화된 뇌의 뉴런 개념 발표

* 퍼셉트론(perceptron)![퍼셉트론](http://www.saedsayad.com/images/Perceptron_3.png)

	* 자동으로 최적의 가중치 학습

	* 가중치 : 뉴런의 출력 신호를 낼지 말지를 결정하기 위해 입력 특성에 곱하는 계수

* 헵의 학습 규칙 : 신경망 모델들의 학습 규칙의 모델이r 되었다.

* 인공신경망 뉴런 (퍼셉트론 수준일 경우)
![인공신경망 뉴런](https://github.com/Deplim/DevNote/blob/master/Image/%EB%89%B4%EB%9F%B0.PNG?raw=true)

	* 활성화함수 : 뉴런의 출력값을 정함.

* 코드 예시
![뉴런 계산](https://github.com/Deplim/DevNote/blob/master/Image/%EB%89%B4%EB%9F%B0%20%EA%B3%84%EC%82%B0.PNG?raw=true)

	* 위에서의 경사하강법은 성형 회귀에서 배웠던 식을 계산하면 이해할 수 있음.

	* 입력값이 0일 때는 w(가중치) 를 아무리 조정해줘도 의미가 없음
![뉴런 계산2](https://github.com/Deplim/DevNote/blob/master/Image/%EB%89%B4%EB%9F%B0%20%EA%B3%84%EC%82%B02.PNG?raw=true)

	* 편향 : 위의 현상을 방지하기 위해서 넣는 늘 한쪽으로 치우쳐진 고정값

	* 입력은 1, 편향은 가중치처럼 조정

* 단층 퍼셉트론으로 해결을 못하는 경우

	* XOR

	* 선으로 구분을 못하는 데이터가 있을 때

	* **다층 퍼셉트론** 을 사용하면 어느정도 해결 가능

	* 이 문제로 인해 인공신경만 연구 느려짐

* parallel distributed processing - explorations in the microstructure of cognition

	* 1986년 
	* 다층 퍼셉트론 제시, 은닉층을 활용하면 선형 분류 판별선을 여러개 그리는 표과를 얻어 xor 문제를 해결할 수 있다고 주장

	* 단점 : 파라미터갯수가 많아지면 적절한 가중치와 편향을 학습하는 것이 어려움

* 역전파 알고리즘
	* 위의 문제 해결
	* 인공지능 연구를 가속화시켜주는 계기

-----------------------------------------

실제 인공신경망의 손실함수는 다차함수이므로 정의하거나 미분하기 어려울 때가 많다.

* 신경망의 목적 : 손실함수가 최솟값일 때의 파라미터를 찾아 올바른 학습 결과를 내는 것

	* 회귀분석이나 로지스틱 회귀와 기본개념이 같다.

	* 단 신경망은 파라미터의 수가 더 많은 편

	* 가중치를 효율적으로 찾기 위한 방법 필요. 그 방법이 역전파

* 역전파 (backpropagation)
	* parallel distributed processing 서적을 통해 공개
	*  뉴런의 가중치를 효율적으로 조정하기 위해, 거꾸로 무엇인가를 전파
	
	
	
