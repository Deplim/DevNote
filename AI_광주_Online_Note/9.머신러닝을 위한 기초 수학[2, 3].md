> [광주 인공지능 사관학교 || 머신러닝을 위한 기초 수학[2, 3]](http://precourse.gj-aischool.com/lectures/10)
	
### Regression
머신러닝의 핵심 : 과거의 관측을 기반으로 새로운 샘플의 결과값을 예측

그래프에 있는 점들을 지나는 함수 모양을 알아내기
* y=ax+b 

* 적합한 가중치와 절편을 찾아 그 파라미터를 통해 실제 결과값과 예측 결괏값이 차이(오차) 가 가능한 작아지는 함수를 찾기

* 목적함수 : 오차의 합계를 나타냄
![목적함수](https://github.com/Deplim/DevNote/blob/master/Image/%EB%AA%A9%EC%A0%81%ED%95%A8%EC%88%98.PNG?raw=true)

* 결국 목적함수를 가장 작아지게 만들어야 하는 것 > 이런것을 "최적화 문제" 라고 한다.

* 경사 하강법 : 목적함수를 최소화하기 위해 사용. 
	도함수의 부호 반대 방향으로 값을 옮겨가면 점점 최소값에 가까워진다는 논리를 기반으로 한다. 

* 갱신 함수
![경사하강법](https://github.com/Deplim/DevNote/blob/master/Image/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.PNG?raw=true)

* 학습률이 너무 크면 오히려 최솟값에서 멀어지는 '발산' 이 일어나고 반대로 너무 작게 잡으면 최솟값에 수렴하기까지 너무 많은 시간이 걸린다.

------------------------------------------

* 최적화해야하는 파라미터가 많은 경우 편미분 사용

* 편미분 : 하나의 변수를 제외한 다른 파라미터들을 상수 취급하고 미분.

* 합성함수의 미분

* 다변수 목적함수 최적화 : 
	각각의 파라미터별로 갱신함수 계산
![다변수 목적함수 최적화](https://github.com/Deplim/DevNote/blob/master/Image/%EB%8B%A4%EB%B3%80%EC%88%98%20%EB%AA%A9%EC%A0%81%ED%95%A8%EC%88%98%20%EC%B5%9C%EC%A0%81%ED%99%94.PNG?raw=true)


* 목적함수의 차수 : 매개변수가 늘어나도 같은 방법으로 갱신식을 도출할 수 있다. > "다항식 회귀"
![갱신함수](https://github.com/Deplim/DevNote/blob/master/Image/%EA%B0%B1%EC%8B%A0%ED%95%A8%EC%88%98.PNG?raw=true)

* 과적합 : 목적함수의 차수는 어떻게든 설정할 수 있지만 너무 크게 설정할 경우 과적합 문제가 생길 수 있다. 
